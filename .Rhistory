margins = c(NA,200,50,NA),
colors = viridis(
n= 256, alpha=1,
begin=0, end=1,
option="viridis"),
fontsize_row = 5,
fontsize_col = 5,
main="Hierarchical Clustering",
ylab = "Towns",
xlab = "Time")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="canberra")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="canberra")
#library(shiny)
#library(shinydashboard)
#library(shinythemes)
#library(shinyWidgets)
library(RColorBrewer)
library(tidyverse)
library(lubridate)
library(geofacet)
library(treemap)
library(ggstatsplot)
library(ggridges)
# Filter the required data for clustering
# remove month = "Annual"
# remove dewlling_type/description = Overall
# year 2018 and onwards due to missing data
# exclude "%region" in description
# Exclude Pioneer as data is incomplete
clus_data <- T3.5 %>%
filter(month != "Annual" &
year > 2017 &
dwelling_type != "Overall" &
!str_detect(Description,"Region|Pioneer|Overall"))
# transform dataset
# convert kwh into numbers
clus_data$kwh_per_acc <- as.numeric(clus_data$kwh_per_acc)
# join month and year into a date
clus_data$date <- parse_date_time(paste(clus_data$year, clus_data$month), orders=c("%Y %m"))
# drop month and year column
clus_data <- subset(clus_data, select=-c(month, year, Region)) %>%
arrange(date)
# pivot wider
clus <- clus_data %>%
pivot_wider(names_from=date, values_from=kwh_per_acc)
# omit na
clus <- na.omit(clus)
clus <- clus %>% relocate(Description, .before = dwelling_type)
# clus_data
# clus
# parameter 1: distance method ("euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski")
# parameter 2: hclust method ("ward.D", "ward.D2","single","complete","average", "mcquitty", "median" or "centroid")
# parameter 3: number of clusters
# parameter 4: seriate (Optimal leaf ordering, Gruvaeus and Wainer, mean, none)
# parameter 5: scale / normalize / percentize (the code for this part is different)
# Remove dwelling type
clus_group1 <- clus[,-c(2)] %>%
group_by(Description) %>%
summarise_each(list(sum))
# making "Description" the row name (index)
row.names(clus_group1) <- clus_group1$Description
# Making it into a matrix
clus_matrix1 <- data.matrix(clus_group1)
# plot
heatmaply(clus_matrix1[,-c(1)],
scale = "column",
dist_method = "euclidean",
hclust_method = "average",
Colv=NA,
seriate = "none",
k_row = 3,
margins = c(NA,200,50,NA),
colors = viridis(
n= 256, alpha=1,
begin=0, end=1,
option="viridis"),
fontsize_row = 5,
fontsize_col = 5,
main="Hierarchical Clustering",
ylab = "Towns",
xlab = "Time")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="canberra")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="manhattan")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="manhattan")
packages = c('tidyverse', 'ggstatsplot', 'psych', 'lubridate', 'ggrepel', 'plotly', "tidyr", "readr")
for(p in packages){
if(!require(p,character.only = T)){
install.packages(p)
}
library(p,character.only = T)
}
library(readr)
T2.3 <- read_csv("data/T2-3.csv")
saveRDS(T2.3, file = "RDS/T2-3.rds")
#T2.6 <- read_csv("data/T2-6.csv")
#saveRDS(T2.6, file = "RDS/T2-6.rds")
T3.4 <- read_csv("data/T3-4.csv")
saveRDS(T3.4, file = "RDS/T3-4.rds")
T3.5 <- read_csv("data/T3-5.csv")
saveRDS(T3.5, file = "RDS/T3-5.rds")
T3.6 <- read_csv("data/T3-6.csv")
saveRDS(T3.6, file = "RDS/T3-6.rds")
T3.7 <- read_csv("data/T3-7.csv")
saveRDS(T3.7, file = "RDS/T3-7.rds")
T3.8 <- read_csv("data/T3-8.csv")
saveRDS(T3.8, file = "RDS/T3-8.rds")
T3.9 <- read_csv("data/T3-9.csv")
saveRDS(T3.9, file = "RDS/T3-9.rds")
# T5.1 <- read_csv("data/T5-1.csv")
# saveRDS(T5.1, file = "RDS/T5-1.rds")
#
# T5.2 <- read_csv("data/T5-2.csv")
# saveRDS(T5.2, file = "RDS/T5-2.rds")
#T5.3 <- read_csv("data/T5-3.csv")
#saveRDS(T5.3, file = "RDS/T5-3.rds")
# T5.4 <- read_csv("data/T5-4.csv")
# saveRDS(T5.4, file = "RDS/T5-4.rds")
#
# T5.5 <- read_csv("data/T5-5.csv")
# saveRDS(T5.4, file = "RDS/T5-5.rds")
#| fig-width: 10
knitr::kable(head(T2.3, 5))
#| fig-width: 10
knitr::kable(head(T2.6, 5))
#| fig-width: 10
knitr::kable(head(T2.6, 5))
library(readr)
T2.3 <- read_csv("data/T2-3.csv")
saveRDS(T2.3, file = "RDS/T2-3.rds")
#T2.6 <- read_csv("data/T2-6.csv")
#saveRDS(T2.6, file = "RDS/T2-6.rds")
T3.4 <- read_csv("data/T3-4.csv")
saveRDS(T3.4, file = "RDS/T3-4.rds")
T3.5 <- read_csv("data/T3-5.csv")
saveRDS(T3.5, file = "RDS/T3-5.rds")
T3.6 <- read_csv("data/T3-6.csv")
saveRDS(T3.6, file = "RDS/T3-6.rds")
T3.7 <- read_csv("data/T3-7.csv")
saveRDS(T3.7, file = "RDS/T3-7.rds")
T3.8 <- read_csv("data/T3-8.csv")
saveRDS(T3.8, file = "RDS/T3-8.rds")
T3.9 <- read_csv("data/T3-9.csv")
saveRDS(T3.9, file = "RDS/T3-9.rds")
# T5.1 <- read_csv("data/T5-1.csv")
# saveRDS(T5.1, file = "RDS/T5-1.rds")
#
# T5.2 <- read_csv("data/T5-2.csv")
# saveRDS(T5.2, file = "RDS/T5-2.rds")
#T5.3 <- read_csv("data/T5-3.csv")
#saveRDS(T5.3, file = "RDS/T5-3.rds")
# T5.4 <- read_csv("data/T5-4.csv")
# saveRDS(T5.4, file = "RDS/T5-4.rds")
#
# T5.5 <- read_csv("data/T5-5.csv")
# saveRDS(T5.4, file = "RDS/T5-5.rds")
#| fig-width: 10
knitr::kable(head(T2.6, 5))
# parameter 1: distance method ("euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski")
# parameter 2: hclust method ("ward.D", "ward.D2","single","complete","average", "mcquitty", "median" or "centroid")
# parameter 3: number of clusters
# parameter 4: seriate (Optimal leaf ordering, Gruvaeus and Wainer, mean, none)
# parameter 5: scale / normalize / percentize (the code for this part is different)
# Remove dwelling type
clus_group1 <- clus[,-c(2)] %>%
group_by(Description) %>%
summarise_each(list(sum))
# making "Description" the row name (index)
row.names(clus_group1) <- clus_group1$Description
# Making it into a matrix
clus_matrix1 <- data.matrix(clus_group1)
# plot
heatmaply(clus_matrix1[,-c(1)],
scale = "column",
dist_method = "euclidean",
hclust_method = "average",
Colv=NA,
seriate = "none",
k_row = 3,
margins = c(NA,200,50,NA),
colors = viridis(
n= 256, alpha=1,
begin=0, end=1,
option="viridis"),
fontsize_row = 5,
fontsize_col = 5,
main="Hierarchical Clustering",
ylab = "Towns",
xlab = "Time")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="manhattan")
runApp('Shiny_App_G9')
# omit na
clus <- na.omit(clus)
clus <- clus %>% relocate(Description, .before = dwelling_type)
clus_group1 <- clus[,-c(2)] %>%
group_by(Description) %>%
summarise_each(list(sum))
# making "Description" the row name (index)
row.names(clus_group1) <- clus_group1$Description
# Making it into a matrix
clus_matrix1 <- data.matrix(clus_group1)
# plot
observeEvent(c(input$k, input$method, input$method2, input$distance),{
output$dendro <- renderPlotly({
heatmaply(clus_matrix1[,-c(1)],
scale = "column",
dist_method = input$distance,
hclust_method = input$method2,
Colv=NA,
seriate = "none",
k_row = input$k,
margins = c(NA,200,50,NA),
colors = viridis(
n= 256, alpha=1,
begin=0, end=1,
option="viridis"),
fontsize_row = 7,
fontsize_col = 7,
main="Hierarchical Clustering",
ylab = "Towns",
xlab = "Time")
})
# clustering <- dist(normalize(clus_group1,-c(1)), method=input$method)
#   # clustering dendex --------------------------------------------------------
#   output$dendextend <- renderDataTable(
#     dend_expend(clustering)[[3]]
#   )
#   # clustering number k ------------------------------------------------------
#   clust2 <- hclust(clustering, method = input$method)
#   num_k <- find_k(clust2)
#   output$numberk <- renderPlot(
#     plot(num_k)
#   )
#
#   # clustering map -----------------------------------------------------------
#   num_clus <- cutree(clust2, k=input$k)
#   clus_hc <- cbind(clus_group1, cluster = as.factor(num_clus))
#
#   clus_hc$Description <- toupper(clus_hc$Description)
#
#   # Preparing the choropleth map
#   mpsz_clus <- left_join(singapore, clus_hc, by = c("PLN_AREA_N" = "Description"))
#     output$map <- renderTmap(
#
#       tm_shape(mpsz_clus)+
#         tmap_options(check.and.fix = TRUE)+
#         tm_fill("cluster", id=paste("PLN_AREA_N"),
#                 style = "pretty",
#                 palette = viridis(input$k)) +
#         tm_borders(alpha = 0.7)
#     )
})
runApp('Shiny_App_G9')
View(clus_data)
runApp('Shiny_App_G9')
# Filter the required data for clustering
# remove month = "Annual"
# remove dewlling_type/description = Overall
# year 2018 and onwards due to missing data
# exclude "%region" in description
# Exclude Pioneer as data is incomplete
clus_data <- T3.5 %>%
filter(month != "Annual" &
year > 2017 &
dwelling_type != "Overall" &
!str_detect(Description,"Region|Pioneer|Overall"))
# transform dataset
# convert kwh into numbers
clus_data$kwh_per_acc <- as.numeric(clus_data$kwh_per_acc)
# join month and year into a date
clus_data$date <- parse_date_time(paste(clus_data$year, clus_data$month), orders=c("%Y %m"))
# drop month and year column
clus_data <- subset(clus_data, select=-c(month, year, Region)) %>%
arrange(date)
# pivot wider
clus <- clus_data %>%
pivot_wider(names_from=date, values_from=kwh_per_acc)
# omit na
clus <- na.omit(clus)
clus <- clus %>% relocate(Description, .before = dwelling_type)
# clus_data
# clus
# parameter 1: distance method ("euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski")
# parameter 2: hclust method ("ward.D", "ward.D2","single","complete","average", "mcquitty", "median" or "centroid")
# parameter 3: number of clusters
# parameter 4: seriate (Optimal leaf ordering, Gruvaeus and Wainer, mean, none)
# parameter 5: scale / normalize / percentize (the code for this part is different)
# Remove dwelling type
clus_group1 <- clus[,-c(2)] %>%
group_by(Description) %>%
summarise_each(list(sum))
# making "Description" the row name (index)
row.names(clus_group1) <- clus_group1$Description
# Making it into a matrix
clus_matrix1 <- data.matrix(clus_group1)
# plot
heatmaply(clus_matrix1[,-c(1)],
scale = "column",
dist_method = "euclidean",
hclust_method = "average",
Colv=NA,
seriate = "none",
k_row = 3,
margins = c(NA,200,50,NA),
colors = viridis(
n= 256, alpha=1,
begin=0, end=1,
option="viridis"),
fontsize_row = 5,
fontsize_col = 5,
main="Hierarchical Clustering",
ylab = "Towns",
xlab = "Time")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="manhattan")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="canberra")
# once user have gotten the clustering method from above, they need to input that to find the optimal number of k
clust2 <- hclust(clustering, method = "mcquitty")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="canberra")
# parameter 1: distance method ("euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski")
# parameter 2: hclust method ("ward.D", "ward.D2","single","complete","average", "mcquitty", "median" or "centroid")
# parameter 3: number of clusters
# parameter 4: seriate (Optimal leaf ordering, Gruvaeus and Wainer, mean, none)
# parameter 5: scale / normalize / percentize (the code for this part is different)
# Remove dwelling type
clus_group1 <- clus[,-c(2)] %>%
group_by(Description) %>%
summarise_each(list(sum))
# making "Description" the row name (index)
row.names(clus_group1) <- clus_group1$Description
# Making it into a matrix
clus_matrix1 <- data.matrix(clus_group1)
# plot
heatmaply(clus_matrix1[,-c(1)],
scale = "column",
dist_method = "euclidean",
hclust_method = "average",
Colv=NA,
seriate = "none",
k_row = 3,
margins = c(NA,200,50,NA),
colors = viridis(
n= 256, alpha=1,
begin=0, end=1,
option="viridis"),
fontsize_row = 5,
fontsize_col = 5,
main="Hierarchical Clustering",
ylab = "Towns",
xlab = "Time")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="canberra")
View(clus_group1)
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="canberra")
runApp('Shiny_App_G9')
runApp('Shiny_App_G9')
runApp('Shiny_App_G9')
runApp('Shiny_App_G9')
runApp('Shiny_App_G9')
runApp('Shiny_App_G9')
packages = c('tidyverse', 'ggstatsplot', 'psych', 'lubridate', 'ggrepel', 'plotly', "tidyr", "readr")
for(p in packages){
if(!require(p,character.only = T)){
install.packages(p)
}
library(p,character.only = T)
}
# Filter the required data for clustering
# remove month = "Annual"
# remove dewlling_type/description = Overall
# year 2018 and onwards due to missing data
# exclude "%region" in description
# Exclude Pioneer as data is incomplete
clus_data <- T3.5 %>%
filter(month != "Annual" &
year > 2017 &
dwelling_type != "Overall" &
!str_detect(Description,"Region|Pioneer|Overall"))
library(readr)
T2.3 <- read_csv("data/T2-3.csv")
saveRDS(T2.3, file = "RDS/T2-3.rds")
#T2.6 <- read_csv("data/T2-6.csv")
#saveRDS(T2.6, file = "RDS/T2-6.rds")
T3.4 <- read_csv("data/T3-4.csv")
saveRDS(T3.4, file = "RDS/T3-4.rds")
T3.5 <- read_csv("data/T3-5.csv")
saveRDS(T3.5, file = "RDS/T3-5.rds")
T3.6 <- read_csv("data/T3-6.csv")
saveRDS(T3.6, file = "RDS/T3-6.rds")
T3.7 <- read_csv("data/T3-7.csv")
saveRDS(T3.7, file = "RDS/T3-7.rds")
T3.8 <- read_csv("data/T3-8.csv")
saveRDS(T3.8, file = "RDS/T3-8.rds")
T3.9 <- read_csv("data/T3-9.csv")
saveRDS(T3.9, file = "RDS/T3-9.rds")
# T5.1 <- read_csv("data/T5-1.csv")
# saveRDS(T5.1, file = "RDS/T5-1.rds")
#
# T5.2 <- read_csv("data/T5-2.csv")
# saveRDS(T5.2, file = "RDS/T5-2.rds")
#T5.3 <- read_csv("data/T5-3.csv")
#saveRDS(T5.3, file = "RDS/T5-3.rds")
# T5.4 <- read_csv("data/T5-4.csv")
# saveRDS(T5.4, file = "RDS/T5-4.rds")
#
# T5.5 <- read_csv("data/T5-5.csv")
# saveRDS(T5.4, file = "RDS/T5-5.rds")
# Filter the required data for clustering
# remove month = "Annual"
# remove dewlling_type/description = Overall
# year 2018 and onwards due to missing data
# exclude "%region" in description
# Exclude Pioneer as data is incomplete
clus_data <- T3.5 %>%
filter(month != "Annual" &
year > 2017 &
dwelling_type != "Overall" &
!str_detect(Description,"Region|Pioneer|Overall"))
# transform dataset
# convert kwh into numbers
clus_data$kwh_per_acc <- as.numeric(clus_data$kwh_per_acc)
# join month and year into a date
clus_data$date <- parse_date_time(paste(clus_data$year, clus_data$month), orders=c("%Y %m"))
# drop month and year column
clus_data <- subset(clus_data, select=-c(month, year, Region)) %>%
arrange(date)
# pivot wider
clus <- clus_data %>%
pivot_wider(names_from=date, values_from=kwh_per_acc)
# omit na
clus <- na.omit(clus)
clus <- clus %>% relocate(Description, .before = dwelling_type)
# clus_data
# clus
# parameter 1: distance method ("euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski")
# parameter 2: hclust method ("ward.D", "ward.D2","single","complete","average", "mcquitty", "median" or "centroid")
# parameter 3: number of clusters
# parameter 4: seriate (Optimal leaf ordering, Gruvaeus and Wainer, mean, none)
# parameter 5: scale / normalize / percentize (the code for this part is different)
# Remove dwelling type
clus_group1 <- clus[,-c(2)] %>%
group_by(Description) %>%
summarise_each(list(sum))
# making "Description" the row name (index)
row.names(clus_group1) <- clus_group1$Description
# Making it into a matrix
clus_matrix1 <- data.matrix(clus_group1)
# plot
heatmaply(clus_matrix1[,-c(1)],
scale = "column",
dist_method = "euclidean",
hclust_method = "average",
Colv=NA,
seriate = "none",
k_row = 3,
margins = c(NA,200,50,NA),
colors = viridis(
n= 256, alpha=1,
begin=0, end=1,
option="viridis"),
fontsize_row = 5,
fontsize_col = 5,
main="Hierarchical Clustering",
ylab = "Towns",
xlab = "Time")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="canberra")
# once user have gotten the clustering method from above, they need to input that to find the optimal number of k
clust2 <- hclust(clustering, method = "mcquitty")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="euclidean")
# parameter: method
# user will need to input the distance calculated method here to determine which clustering method is optimal
clus_group1 <- clus[,-c(2)] %>%
group_by(Description) %>%
summarise_each(list(sum))
library(dendextend)
clustering <- dist(normalize(clus_group1,-c(1)), method="euclidean")
